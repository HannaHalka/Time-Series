{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assignment 1\n",
    "Halka Hanna\n",
    "\n",
    "Selecting 3 time series datasets"
   ],
   "id": "1c6c555ce070bd52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "d3af5c5e0099c52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Common Helper Functions",
   "id": "ca870bd603808318"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check on null values in columns.",
   "id": "34d2aab33862ffe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_non_null(df):\n",
    "    cols_with_missing = [col for col in df.columns if df[col].isnull().any()]\n",
    "    print('number of missing columns: ', len(cols_with_missing))"
   ],
   "id": "7a253372519a7725",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check on data for time series.",
   "id": "a1c59b1b40d7da8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_date_valid(df_col, step):\n",
    "    expected = pd.date_range(df_col.min(), df_col.max(), freq=step)\n",
    "    missing = expected.difference(df_col)\n",
    "    print(missing)"
   ],
   "id": "c88a39713c4779ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Minimum Temperatures",
   "id": "7eb52ffe6a651470"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Daily Minimum Temperatures in Melbourne. The dataset collect data from `1981-01-01` to `1990-12-31`.\n",
    "\n",
    "| Date       | mean-tempt |\n",
    "|-------------|------------|\n",
    "| 1981-01-01  | 20.7       |\n",
    "| 1981-01-02  | 17.9       |\n",
    "| 1981-01-03  | 18.8       |\n",
    "| ...         | ...        |\n",
    "| 1990-12-31  | 13.0       |\n",
    "\n",
    "3650 rows Ã— 2 columns\n",
    "\n",
    "Data from Kaggle: https://www.kaggle.com/datasets/paulbrabban/daily-minimum-temperatures-in-melbourne"
   ],
   "id": "6bc9940418768654"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_tempt = pd.read_csv('data/daily-minimum-temperatures-in-me.csv')\n",
    "df_tempt = df_tempt.rename(columns={'Daily minimum temperatures in Melbourne, Australia, 1981-1990': 'mean-tempt'})\n",
    "df_tempt['Date'] = pd.to_datetime(df_tempt['Date'])\n",
    "df_tempt = df_tempt.sort_values(['Date'])\n",
    "is_non_null(df_tempt)\n",
    "is_date_valid(df_tempt['Date'], 'D')"
   ],
   "id": "8512368c5495fe1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since we don't have `1984-12-31`, `1988-12-31` we will add it by hands, mean temperature will be the average of nearest neighbor.",
   "id": "49481a47a71b2fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dates = ['1984-12-30', '1985-01-01', '1988-12-30', '1989-01-01']\n",
    "helper_dates = df_tempt[df_tempt['Date'].isin(dates)]\n",
    "\n",
    "first_average = (16.4 + 13.3) / 2\n",
    "second_average = (14.1 + 14.3) / 2\n",
    "\n",
    "helper_df = pd.DataFrame({\n",
    "    'Date': ['1984-12-31', '1988-12-31'],\n",
    "    'mean-tempt': [first_average, second_average]\n",
    "})\n",
    "\n",
    "df_tempt = pd.concat([df_tempt, helper_df], ignore_index=True)\n",
    "df_tempt['Date'] = pd.to_datetime(df_tempt['Date'])\n",
    "df_tempt = df_tempt.sort_values(['Date'])\n",
    "is_date_valid(df_tempt['Date'], 'D')"
   ],
   "id": "cad4acf00715cfaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And last but not least duplicates.",
   "id": "2ba4b6c25069af14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "duplicates = df_tempt[df_tempt.duplicated(subset=['Date'])]\n",
    "duplicates\n"
   ],
   "id": "2a0f47a2ffa1b960",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Trends in CO2",
   "id": "72c1d1bebc837a5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Weekly $CO_2$ average. The dataset collect data from `1974-05-19` to `2025-10-12`.\n",
    "\n",
    "| year | month | day | decimal   | average | ndays | 1 year ago | 10 years ago | increase since 1800 |\n",
    "|------|--------|-----|-----------|----------|--------|-------------|---------------|----------------------|\n",
    "| 1974 | 5      | 19  | 1974.3795 | 333.37   | 5      | -999.99     | -999.99       | 50.40                |\n",
    "| 1974 | 5      | 26  | 1974.3986 | 332.95   | 6      | -999.99     | -999.99       | 50.06                |\n",
    "| 1974 | 6      | 2   | 1974.4178 | 332.35   | 5      | -999.99     | -999.99       | 49.60                |\n",
    "| ...  | ...    | ... | ...       | ...      | ...    | ...         | ...           | ...                  |\n",
    "| 2025 | 10     | 12  | 2025.7795 | 424.81   | 6      | 422.54      | 398.46        | 148.18               |\n",
    "\n",
    "---\n",
    "\n",
    "Our date now has format:\n",
    "\n",
    "| year | month | day |\n",
    "|------|--------|-----|\n",
    "| 1974 | 5      | 19  |\n",
    "| 1974 | 5      | 26  |\n",
    "| 1974 | 6      | 2   |\n",
    "| ...  | ...    | ... |\n",
    "| 2025 | 10     | 12  |\n",
    "\n",
    "But we need one column with date time format, so we combine them into one column `datetime`.\n",
    "\n",
    "| datetime   |\n",
    "|------------|\n",
    "| 1974-05-19 |\n",
    "| 1974-05-26 |\n",
    "| 1974-06-02 |\n",
    "| ...        |\n",
    "| 2025-10-12 |\n",
    "\n",
    "---\n",
    "\n",
    "So our table look like:\n",
    "\n",
    "| datetime   | decimal   | average | ndays | 1 year ago | 10 years ago | increase since 1800 |\n",
    "|------------|-----------|----------|--------|-------------|---------------|----------------------|\n",
    "| 1974-05-19 | 1974.3795 | 333.37   | 5      | -999.99     | -999.99       | 50.40                |\n",
    "| 1974-05-26 | 1974.3986 | 332.95   | 6      | -999.99     | -999.99       | 50.06                |\n",
    "| 1974-06-02 | 1974.4178 | 332.35   | 5      | -999.99     | -999.99       | 49.60                |\n",
    "| ...        | ...       | ...      | ...    | ...         | ...           | ...                  |\n",
    "| 2025-10-12 | 2025.7795 | 424.81   | 6      | 422.54      | 398.46        | 148.18               |\n",
    "\n",
    "2683 rows Ã— 7 columns\n",
    "\n",
    "Data from Global Monitoring Laboratory: https://gml.noaa.gov/ccgg/trends/data.html?utm_source=chatgpt.com"
   ],
   "id": "4139bd09a5b9ffe0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_co2 = pd.read_csv('data/co2_weekly_mlo.csv')\n",
    "df_co2['datetime'] = df_co2['year'].astype(str) + \"-\" + df_co2['month'].astype(str) + \"-\" + df_co2['day'].astype(str)\n",
    "df_co2 = df_co2[['datetime', 'decimal',\t'average', 'ndays', '1 year ago', '10 years ago', 'increase since 1800']]\n",
    "df_co2['datetime'] = pd.to_datetime(df_co2['datetime'])\n",
    "df_co2 = df_co2.sort_values(['datetime'])\n",
    "is_non_null(df_co2)\n",
    "is_date_valid(df_co2['datetime'], 'W')\n",
    "\n",
    "print('\\n Duplicates')\n",
    "duplicates = df_co2[df_co2.duplicated(subset=['datetime'])]\n",
    "duplicates"
   ],
   "id": "60210ac9df558f11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_co2[df_co2['average'] == -999.99]",
   "id": "ab6cb55cba66b2a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_co2['average'] = df_co2['average'].replace(-999.99, pd.NA)\n",
    "mask = df_co2['average'] == 'null'\n",
    "\n",
    "indices = df_co2[mask].index\n",
    "\n",
    "for i in indices:\n",
    "    if i > 0 and i < len(df_co2) - 1:\n",
    "        prev_val = df_co2.loc[i - 1, 'average']\n",
    "        next_val = df_co2.loc[i + 1, 'average']\n",
    "        df_co2.loc[i, 'average'] = (prev_val + next_val) / 2\n"
   ],
   "id": "95512d09d62343e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_co2[df_co2['average'] == -999.99]",
   "id": "97503f91453cecc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_co2[df_co2['average'] == pd.NA]",
   "id": "a76b7b44d1859a58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Energy Consumption",
   "id": "19762f3df8eb133e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hourly Energy Consumption. The dataset collect data from `1981-01-01 01:00:00` to `1990-12-31 00:00:00`.\n",
    "\n",
    "| Datetime            | AEP_MW  |\n",
    "|---------------------|---------|\n",
    "| 2004-10-31 01:00:00 | 12379.0 |\n",
    "| 2004-10-31 02:00:00 | 11935.0 |\n",
    "| 2004-10-31 03:00:00 | 11692.0 |\n",
    "| ...                 | ...     |\n",
    "| 2018-08-03 00:00:00 | 14809.0 |\n",
    "\n",
    "121273 rows Ã— 2 columns\n",
    "\n",
    "Data from Kaggle: https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption?utm_source=chatgpt.com"
   ],
   "id": "47ce028c879a6f4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_aep = pd.read_csv('data/AEP_hourly.csv')\n",
    "df_aep['Datetime'] = pd.to_datetime(df_aep['Datetime'])\n",
    "df_aep = df_aep.sort_values(['Datetime'])\n",
    "is_non_null(df_aep)\n",
    "is_date_valid(df_aep['Datetime'], 'H')"
   ],
   "id": "add7df2fe0e194e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hear we have a lot of missing data. Since we don't have much time we will use simple mean but we will fixe it in future.",
   "id": "b0cc61831ae15723"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "helper_df = pd.DataFrame({\n",
    "    'Datetime': ['2004-10-31 02:00:00', '2005-04-03 03:00:00',\n",
    "             '2005-10-30 02:00:00', '2006-04-02 03:00:00',\n",
    "             '2006-10-29 02:00:00', '2007-03-11 03:00:00',\n",
    "             '2007-11-04 02:00:00', '2008-03-09 03:00:00',\n",
    "             '2008-11-02 02:00:00', '2009-03-08 03:00:00',\n",
    "             '2009-11-01 02:00:00', '2010-03-14 03:00:00',\n",
    "             '2010-11-07 02:00:00', '2010-12-10 00:00:00',\n",
    "             '2011-03-13 03:00:00', '2011-11-06 02:00:00',\n",
    "             '2012-03-11 03:00:00', '2012-11-04 02:00:00',\n",
    "             '2012-12-06 04:00:00', '2013-03-10 03:00:00',\n",
    "             '2013-11-03 02:00:00', '2014-03-09 03:00:00',\n",
    "             '2014-03-11 14:00:00', '2015-03-08 03:00:00',\n",
    "             '2016-03-13 03:00:00', '2017-03-12 03:00:00',\n",
    "             '2018-03-11 03:00:00'],\n",
    "    'AEP_MW': 0\n",
    "})\n",
    "\n",
    "df_aep = pd.concat([df_aep, helper_df], ignore_index=True)\n",
    "df_aep['Datetime'] = pd.to_datetime(df_aep['Datetime'])\n",
    "df_aep = df_aep.sort_values('Datetime')\n",
    "\n",
    "missing = pd.to_datetime([\n",
    "    '2004-10-31 02:00:00','2005-04-03 03:00:00','2005-10-30 02:00:00',\n",
    "    '2006-04-02 03:00:00','2006-10-29 02:00:00','2007-03-11 03:00:00',\n",
    "    '2007-11-04 02:00:00','2008-03-09 03:00:00','2008-11-02 02:00:00',\n",
    "    '2009-03-08 03:00:00','2009-11-01 02:00:00','2010-03-14 03:00:00',\n",
    "    '2010-11-07 02:00:00','2010-12-10 00:00:00','2011-03-13 03:00:00',\n",
    "    '2011-11-06 02:00:00','2012-03-11 03:00:00','2012-11-04 02:00:00',\n",
    "    '2012-12-06 04:00:00','2013-03-10 03:00:00','2013-11-03 02:00:00',\n",
    "    '2014-03-09 03:00:00','2014-03-11 14:00:00','2015-03-08 03:00:00',\n",
    "    '2016-03-13 03:00:00','2017-03-12 03:00:00','2018-03-11 03:00:00'\n",
    "])\n",
    "\n",
    "df = df_aep.sort_values('Datetime').set_index('Datetime')\n",
    "df2 = df.reindex(df.index.union(missing)).sort_index()\n",
    "\n",
    "prev = df2['AEP_MW'].ffill()\n",
    "next = df2['AEP_MW'].bfill()\n",
    "\n",
    "avg_at_missing = ((prev + next) / 2).loc[missing]\n",
    "\n",
    "df2.loc[missing, 'AEP_MW'] = avg_at_missing\n",
    "df_filled = df2.reset_index()\n",
    "\n",
    "df_aep = df_filled.copy()\n",
    "df_aep = df_aep.rename(columns={'index': 'Datetime'})\n",
    "is_non_null(df_aep)\n",
    "is_date_valid(df_aep['Datetime'], 'H')"
   ],
   "id": "81400ba120b57f7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And duplicates, hear we have four duplicates. For now we will just delete them.",
   "id": "b1866e5a58a2e756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('\\n Duplicates')\n",
    "duplicates = df_aep[df_aep.duplicated(subset=['Datetime'])]\n",
    "duplicates"
   ],
   "id": "6229277e922578e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_aep = df_aep.drop_duplicates(subset=['Datetime'])\n",
    "print('\\n Duplicates')\n",
    "duplicates = df_aep[df_aep.duplicated(subset=['Datetime'])]\n",
    "duplicates"
   ],
   "id": "f004fcf4106c2e86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_aep[df_aep['AEP_MW'] == 0]",
   "id": "5d6866ed30887ba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_aep['AEP_MW'] = df_aep['AEP_MW'].replace(0, pd.NA)\n",
    "mask = df_aep['AEP_MW'] == 'null'\n",
    "\n",
    "indices = df_aep[mask].index\n",
    "\n",
    "for i in indices:\n",
    "    if i > 0 and i < len(df_aep) - 1:\n",
    "        prev_val = df_co2.loc[i - 1, 'AEP_MW']\n",
    "        next_val = df_co2.loc[i + 1, 'AEP_MW']\n",
    "        df_co2.loc[i, 'AEP_MW'] = (prev_val + next_val) / 2"
   ],
   "id": "e3dc33800d71b4c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_aep[df_aep['AEP_MW'] == 0]",
   "id": "6b89846ec09e0659",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save to files",
   "id": "7bd613bb3c4ad181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "df_tempt.to_csv('../data/daily_tempt.csv', index=False)\n",
    "df_co2.to_csv('../data/weekly_co2.csv', index=False)\n",
    "df_aep.to_csv('../data/hourly_aep.csv', index=False)"
   ],
   "id": "ffc5d87e82e83857",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
